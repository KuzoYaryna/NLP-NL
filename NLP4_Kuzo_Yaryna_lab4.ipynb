{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef860275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        0.         0.         0.         0.         0.\n",
      "  0.5        0.5        0.         0.         0.5        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.40824829 0.40824829 0.         0.40824829 0.\n",
      "  0.         0.         0.40824829 0.         0.         0.40824829\n",
      "  0.40824829 0.        ]\n",
      " [0.         0.40824829 0.40824829 0.         0.40824829 0.\n",
      "  0.         0.         0.40824829 0.         0.         0.40824829\n",
      "  0.40824829 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.40824829 0.40824829 0.         0.40824829 0.\n",
      "  0.         0.         0.40824829 0.         0.         0.40824829\n",
      "  0.40824829 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.\n",
      "  0.5        0.5        0.         0.         0.5        0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.\n",
      "  0.5        0.5        0.         0.         0.5        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.40824829 0.40824829 0.         0.40824829 0.\n",
      "  0.         0.         0.40824829 0.         0.         0.40824829\n",
      "  0.40824829 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.\n",
      "  0.5        0.5        0.         0.         0.5        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.40824829 0.40824829 0.         0.40824829 0.\n",
      "  0.         0.         0.40824829 0.         0.         0.40824829\n",
      "  0.40824829 0.        ]\n",
      " [0.         0.40824829 0.40824829 0.         0.40824829 0.\n",
      "  0.         0.         0.40824829 0.         0.         0.40824829\n",
      "  0.40824829 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def ppmi_weighting(docs):\n",
    "   \n",
    "    vectorizer = CountVectorizer(tokenizer=lambda text: text.split(), binary=True)\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    co_occurrence_matrix = (X.T * X)\n",
    "\n",
    "   \n",
    "    word_counts = np.array(X.sum(axis=0)).squeeze()\n",
    "    total_word_count = word_counts.sum()\n",
    "\n",
    "    \n",
    "    ppmi_matrix = np.zeros(co_occurrence_matrix.shape)\n",
    "\n",
    "   \n",
    "    num_docs = len(docs)\n",
    "    for i, (word1, idx1) in enumerate(vectorizer.vocabulary_.items()):\n",
    "        for j, (word2, idx2) in enumerate(vectorizer.vocabulary_.items()):\n",
    "            co_occurrences = co_occurrence_matrix[idx1, idx2]\n",
    "            if co_occurrences == 0:\n",
    "                ppmi_matrix[idx1, idx2] = 0\n",
    "            else:\n",
    "                word1_count = word_counts[idx1]\n",
    "                word2_count = word_counts[idx2]\n",
    "                pmi = np.log((co_occurrences * num_docs) / (word1_count * word2_count))\n",
    "                ppmi = max(0, pmi)\n",
    "                ppmi_matrix[idx1, idx2] = ppmi\n",
    "\n",
    "   \n",
    "    ppmi_matrix = normalize(ppmi_matrix, norm='l2', axis=1)\n",
    "\n",
    "    return ppmi_matrix\n",
    "\n",
    "# Example usage:\n",
    "docs = [\n",
    "    \"This is a sample document containing some words.\",\n",
    "    \"Another document with different words but containing some similar words.\"\n",
    "]\n",
    "\n",
    "ppmi_matrix = ppmi_weighting(docs)\n",
    "print(ppmi_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21320748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['Another', 'This', 'a', 'but', 'containing', 'different', 'document', 'is', 'sample', 'similar', 'some', 'with', 'words', 'words.']\n",
      "PPMI Matrix:\n",
      "[[0.         0.         0.         0.5        0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.5\n",
      "  0.5        0.        ]\n",
      " [0.         0.         0.57735027 0.         0.         0.\n",
      "  0.         0.57735027 0.57735027 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.57735027 0.         0.         0.         0.\n",
      "  0.         0.57735027 0.57735027 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.4472136  0.         0.         0.         0.         0.4472136\n",
      "  0.         0.         0.         0.4472136  0.         0.4472136\n",
      "  0.4472136  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.4472136  0.         0.         0.4472136  0.         0.\n",
      "  0.         0.         0.         0.4472136  0.         0.4472136\n",
      "  0.4472136  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.57735027 0.57735027 0.         0.         0.\n",
      "  0.         0.         0.57735027 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.57735027 0.57735027 0.         0.         0.\n",
      "  0.         0.57735027 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.57735027 0.         0.57735027\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.57735027 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.         0.         0.5        0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.        ]\n",
      " [0.4472136  0.         0.         0.4472136  0.         0.4472136\n",
      "  0.         0.         0.         0.4472136  0.         0.4472136\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def ppmi_weighting(docs, window_size=5):\n",
    "    \n",
    "    tokenized_docs = [doc.split() for doc in docs]\n",
    "    co_occurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for doc in tokenized_docs:\n",
    "        for i, word in enumerate(doc):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(doc), i + window_size + 1)\n",
    "            context = doc[start:end]\n",
    "            for j in range(len(context)):\n",
    "                if context[j] != word:\n",
    "                    co_occurrence_matrix[word][context[j]] += 1\n",
    "\n",
    "   \n",
    "    word_counts = defaultdict(int)\n",
    "    for doc in tokenized_docs:\n",
    "        for word in doc:\n",
    "            word_counts[word] += 1\n",
    "    total_word_count = sum(word_counts.values())\n",
    "\n",
    "  \n",
    "    ppmi_matrix = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "   \n",
    "    num_docs = len(docs)\n",
    "    for word, context in co_occurrence_matrix.items():\n",
    "        for co_word, co_count in context.items():\n",
    "            if co_count == 0:\n",
    "                ppmi_matrix[word][co_word] = 0\n",
    "            else:\n",
    "                pmi = np.log((co_count * num_docs) / (word_counts[word] * word_counts[co_word]))\n",
    "                ppmi = max(0, pmi)\n",
    "                ppmi_matrix[word][co_word] = ppmi\n",
    "\n",
    "    \n",
    "    vocab = sorted(word_counts.keys())\n",
    "    ppmi_array = np.zeros((len(vocab), len(vocab)))\n",
    "    for i, word in enumerate(vocab):\n",
    "        for j, co_word in enumerate(vocab):\n",
    "            ppmi_array[i][j] = ppmi_matrix[word][co_word]\n",
    "\n",
    "   \n",
    "    ppmi_array = normalize(ppmi_array, norm='l2', axis=1)\n",
    "\n",
    "    return ppmi_array, vocab\n",
    "\n",
    "# Example usage:\n",
    "docs = [\n",
    "    \"This is a sample document containing some words.\",\n",
    "    \"Another document with different words but containing some similar words.\"\n",
    "]\n",
    "\n",
    "ppmi_matrix, vocab = ppmi_weighting(docs, window_size=5)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"PPMI Matrix:\")\n",
    "print(ppmi_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab5a5745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['Another', 'This', 'a', 'but', 'containing', 'different', 'document', 'is', 'sample', 'similar', 'some', 'with', 'words', 'words.']\n",
      "PPMI Matrix:\n",
      "[[0.         0.         0.         0.49642982 0.         0.49642982\n",
      "  0.11928842 0.         0.         0.         0.         0.49642982\n",
      "  0.49642982 0.        ]\n",
      " [0.         0.         0.56654895 0.         0.13613753 0.\n",
      "  0.13613753 0.56654895 0.56654895 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.55633203 0.         0.         0.13368248 0.\n",
      "  0.13368248 0.55633203 0.55633203 0.         0.13368248 0.\n",
      "  0.         0.13368248]\n",
      " [0.43722925 0.         0.         0.         0.10506296 0.43722925\n",
      "  0.10506296 0.         0.         0.43722925 0.10506296 0.43722925\n",
      "  0.43722925 0.10506296]\n",
      " [0.         0.21821789 0.21821789 0.21821789 0.         0.21821789\n",
      "  0.43643578 0.21821789 0.21821789 0.21821789 0.43643578 0.21821789\n",
      "  0.21821789 0.43643578]\n",
      " [0.43966253 0.         0.         0.43966253 0.10564766 0.\n",
      "  0.10564766 0.         0.         0.43966253 0.10564766 0.43966253\n",
      "  0.43966253 0.        ]\n",
      " [0.2773501  0.2773501  0.2773501  0.2773501  0.5547002  0.2773501\n",
      "  0.         0.2773501  0.2773501  0.         0.         0.2773501\n",
      "  0.2773501  0.        ]\n",
      " [0.         0.56137077 0.56137077 0.         0.13489325 0.\n",
      "  0.13489325 0.         0.56137077 0.         0.13489325 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.55633203 0.55633203 0.         0.13368248 0.\n",
      "  0.13368248 0.55633203 0.         0.         0.13368248 0.\n",
      "  0.         0.13368248]\n",
      " [0.         0.         0.         0.56137077 0.13489325 0.56137077\n",
      "  0.         0.         0.         0.         0.13489325 0.\n",
      "  0.56137077 0.13489325]\n",
      " [0.         0.         0.25       0.25       0.5        0.25\n",
      "  0.         0.25       0.25       0.25       0.         0.25\n",
      "  0.25       0.5       ]\n",
      " [0.48951305 0.         0.         0.48951305 0.11762637 0.48951305\n",
      "  0.11762637 0.         0.         0.         0.11762637 0.\n",
      "  0.48951305 0.        ]\n",
      " [0.43722925 0.         0.         0.43722925 0.10506296 0.43722925\n",
      "  0.10506296 0.         0.         0.43722925 0.10506296 0.43722925\n",
      "  0.         0.10506296]\n",
      " [0.         0.         0.2773501  0.2773501  0.5547002  0.\n",
      "  0.         0.         0.2773501  0.2773501  0.5547002  0.\n",
      "  0.2773501  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def ppmi_weighting(docs, window_size=5):\n",
    "    \n",
    "    tokenized_docs = [doc.split() for doc in docs]\n",
    "    co_occurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for doc in tokenized_docs:\n",
    "        for i, word in enumerate(doc):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(doc), i + window_size + 1)\n",
    "            context = doc[start:end]\n",
    "            for j in range(len(context)):\n",
    "                if context[j] != word:\n",
    "                    co_occurrence_matrix[word][context[j]] += 1\n",
    "\n",
    "   \n",
    "    word_counts = defaultdict(int)\n",
    "    for doc in tokenized_docs:\n",
    "        for word in doc:\n",
    "            word_counts[word] += 1\n",
    "    total_word_count = sum(word_counts.values())\n",
    "\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        word_counts[word] = 1 + np.log(count)  \n",
    "   \n",
    "    ppmi_matrix = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "   \n",
    "    num_docs = len(docs)\n",
    "    for word, context in co_occurrence_matrix.items():\n",
    "        for co_word, co_count in context.items():\n",
    "            if co_count == 0:\n",
    "                ppmi_matrix[word][co_word] = 0\n",
    "            else:\n",
    "                pmi = np.log((co_count * num_docs) / (word_counts[word] * word_counts[co_word]))\n",
    "                ppmi = max(0, pmi)\n",
    "                ppmi_matrix[word][co_word] = ppmi\n",
    "\n",
    "   \n",
    "    vocab = sorted(word_counts.keys())\n",
    "    ppmi_array = np.zeros((len(vocab), len(vocab)))\n",
    "    for i, word in enumerate(vocab):\n",
    "        for j, co_word in enumerate(vocab):\n",
    "            ppmi_array[i][j] = ppmi_matrix[word][co_word]\n",
    "\n",
    "   \n",
    "    ppmi_array = normalize(ppmi_array, norm='l2', axis=1)\n",
    "\n",
    "    return ppmi_array, vocab\n",
    "\n",
    "# Example usage:\n",
    "docs = [\n",
    "    \"This is a sample document containing some words.\",\n",
    "    \"Another document with different words but containing some similar words.\"\n",
    "]\n",
    "\n",
    "ppmi_matrix, vocab = ppmi_weighting(docs, window_size=5)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"PPMI Matrix:\")\n",
    "print(ppmi_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
